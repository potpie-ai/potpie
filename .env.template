isDevelopmentMode=enabled
ENV=development
OPENAI_API_KEY=
OLLAMA_API_KEY=ollama  # optional placeholder when using local Ollama
POSTGRES_SERVER=postgresql://postgres:mysecretpassword@localhost:5432/momentum
NEO4J_URI=bolt://127.0.0.1:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=mysecretpassword
REDISHOST=127.0.0.1
REDISPORT=6379
BROKER_URL=redis://127.0.0.1:6379/0
CELERY_QUEUE_NAME=dev
defaultUsername=defaultuser

# Distributed Parsing Configuration (for C/C++/C# repositories)
# Enable distributed parsing to parallelize large repository parsing across multiple workers
# When enabled, repositories are divided into directory-based work units and processed in parallel
# Default: false (uses sequential parsing)
USE_DISTRIBUTED_PARSING=true
# Number of threads per worker for parallel file parsing (default: 15)
PARSING_WORKER_THREADS=15
# Maximum number of files per work unit (hard limit)
# Recommended: 2000 for 4GB pods, 3000 for 6GB pods, 5000 for 8GB+ pods
# Lower values = more work units, better parallelism, lower memory per task
# Default for production (4GB pods with 1 worker per pod): 2000
MAX_FILES_PER_WORK_UNIT=2000
# Target number of files per work unit for balanced distribution
# The algorithm aims to create work units close to this size for optimal load balancing
# Should be slightly lower than MAX_FILES_PER_WORK_UNIT (e.g., 85-90% of MAX)
# Default: 1750 (87.5% of default MAX)
TARGET_FILES_PER_WORK_UNIT=1750
# Minimum number of files per work unit
# Prevents creating too many tiny work units that would add overhead
# Should be small enough to allow flexibility but large enough to be meaningful
# Default: 100
MIN_FILES_PER_WORK_UNIT=100
# Neo4j batch size for distributed parsing (default: 1000, lower than sequential to reduce memory pressure)
NEO4J_BATCH_SIZE_DISTRIBUTED=1000
# Batch size for processing cross-directory references (default: 10000)
# This controls how many references are processed before writing edges to Neo4j
CROSS_REF_BATCH_SIZE=10000
# Neo4j batch sizes for graph cleanup (two-phase deletion to avoid OOM)
# Phase 1: Delete relationships in batches (default: 5000)
NEO4J_DELETE_REL_BATCH_SIZE=5000
# Phase 2: Delete nodes in batches (default: 1000)
NEO4J_DELETE_NODE_BATCH_SIZE=1000

# Resumable Parsing Configuration
# Enable resumable parsing to allow resuming failed or interrupted parsing runs
# When enabled, parsing can continue from where it left off instead of starting from scratch
# Default: true
ENABLE_RESUMABLE_PARSING=true
# Enable Neo4j bootstrap to resume parsing runs that started before the resumable system was deployed
# This detects partial parsing state in Neo4j and reconstructs work unit tracking
# Default: true
ENABLE_NEO4J_BOOTSTRAP=true
# Maximum number of times to attempt bootstrapping from Neo4j state
# Default: 3
MAX_BOOTSTRAP_ATTEMPTS=3

PROJECT_PATH=projects #repositories will be downloaded/cloned to this path on your system.
INFERENCE_MODEL=openai/gpt-4.1-mini
CHAT_MODEL=openai/gpt-4o
# Optional overrides for OpenAI-compatible endpoints (e.g., Azure, Ollama, Mistral)
LLM_API_BASE=
LLM_API_VERSION=
# Optional capability overrides for custom providers
# Set to "true"/"false" (1/0 also accepted) to force specific behaviour
LLM_SUPPORTS_PYDANTIC=
LLM_SUPPORTS_STREAMING=
LLM_SUPPORTS_VISION=
LLM_SUPPORTS_TOOL_PARALLELISM=
# following are for production mode
GCP_PROJECT=

# Multimodal Feature Flag
# Controls availability of image upload and multimodal AI functionality
# Possible values:
#   "auto" (default) - Enable automatically when all GCP vars present
#   "enabled" - Force enable (requires GCP vars)
#   "disabled" - Force disable regardless of GCP vars
isMultimodalEnabled=auto
# Required for multimodal functionality (when enabled)
OBJECT_STORAGE_PROVIDER=gcs
# Storage provider selection: "s3", "gcs", "azure", or "auto" (default: auto)

# GCS config
GCS_PROJECT_ID=
GCS_BUCKET_NAME=

# GCS S3 Interoperability HMAC Keys (required for boto3 access)
GCS_HMAC_ACCESS_KEY=
GCS_HMAC_SECRET_KEY=

# S3 config
S3_BUCKET_NAME=
AWS_REGION=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# Azure Blob Storage config
AZURE_ACCOUNT_NAME=
AZURE_ACCOUNT_KEY=
AZURE_CONTAINER_NAME=

GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
FIREBASE_SERVICE_ACCOUNT=
KNOWLEDGE_GRAPH_URL=
GITHUB_APP_ID=
GITHUB_PRIVATE_KEY=
GH_TOKEN_LIST=  # Comma-separated GitHub PAT tokens for github.com (e.g., ghp_token1,ghp_token2)
TRANSACTION_EMAILS_ENABLED=
EMAIL_FROM_ADDRESS=
RESEND_API_KEY=
ANTHROPIC_API_KEY=
OPENROUTER_API_KEY=
POSTHOG_API_KEY=
POSTHOG_HOST=
FIRECRAWL_API_KEY=

# GitHub Authentication Configuration
# GH_TOKEN_LIST: Personal Access Tokens for GitHub.com (comma-separated for token pool)
# GITHUB_APP_ID + GITHUB_PRIVATE_KEY: GitHub App credentials (recommended for production)
# CODE_PROVIDER_TOKEN: Token for self-hosted Git servers (GitBucket, GitLab, etc.)
# CODE_PROVIDER_BASE_URL: API base URL for self-hosted Git servers

# Optional: Git provider configuration for self-hosted instances
# Supported providers: github, gitbucket, gitlab, bitbucket, local
CODE_PROVIDER=github  # Options: github, gitlab, gitbucket, local
CODE_PROVIDER_BASE_URL=  # e.g., http://localhost:8080/api/v3 for GitBucket, /path/to/repo for local
CODE_PROVIDER_TOKEN=  # PAT for self-hosted Git server (not needed for local)

# For local provider:
# CODE_PROVIDER=local
# CODE_PROVIDER_BASE_URL=/path/to/local/repository

# For GitHub:
# CODE_PROVIDER=github
# CODE_PROVIDER_BASE_URL=https://api.github.com  # Optional, has default
# CODE_PROVIDER_TOKEN=ghp_xxxxx

# For GitBucket:
# CODE_PROVIDER=gitbucket
# CODE_PROVIDER_BASE_URL=http://localhost:8080/api/v3  # Required
# CODE_PROVIDER_TOKEN=your_token

# For tests
# create a private repo named "potpie-private-test-repo"
# create a file "private_file.txt" with exact contents "This is a private file" in the same repo
# add repo url without github.com
PRIVATE_TEST_REPO_NAME=<yourGithubUsername>/potpie-private-test-repo
