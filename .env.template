isDevelopmentMode=enabled
ENV=development
OPENAI_API_KEY=
OLLAMA_API_KEY=ollama  # optional placeholder when using local Ollama
POSTGRES_SERVER=postgresql://postgres:mysecretpassword@localhost:5432/momentum
NEO4J_URI=bolt://127.0.0.1:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=mysecretpassword
REDISHOST=127.0.0.1
REDISPORT=6379
BROKER_URL=redis://127.0.0.1:6379/0
CELERY_QUEUE_NAME=dev
defaultUsername=defaultuser
PROJECT_PATH=projects #repositories will be downloaded/cloned to this path on your system.
INFERENCE_MODEL=openai/gpt-4.1-mini
CHAT_MODEL=openai/gpt-4o
# Optional overrides for OpenAI-compatible endpoints (e.g., Azure, Ollama, Mistral)
LLM_API_BASE=
LLM_API_VERSION=
# Optional capability overrides for custom providers
# Set to "true"/"false" (1/0 also accepted) to force specific behaviour
LLM_SUPPORTS_PYDANTIC=
LLM_SUPPORTS_STREAMING=
LLM_SUPPORTS_VISION=
LLM_SUPPORTS_TOOL_PARALLELISM=
# following are for production mode
GCP_PROJECT=

# Context and history management (Phase 2: token- and model-aware limits)
# HISTORY_TOKEN_BUDGET: max tokens for history when model context unknown (default: 30000)
# HISTORY_TOKEN_BUDGET_RATIO: fraction of model context window for history (default: 0.75)
# HISTORY_MESSAGE_CAP: max messages in ctx.history when using message-count proxy (default: 50)
# HISTORY_TOKEN_BUDGET=
# HISTORY_TOKEN_BUDGET_RATIO=0.75
# HISTORY_MESSAGE_CAP=50

# Phase 3: Persist compressed history per conversation (cross-request)
# CONTEXT_MANAGEMENT_USE_PERSISTED_COMPRESSED_HISTORY: enable conversation-scoped store (default: true)
# COMPRESSED_HISTORY_TTL_SECONDS: TTL for in-memory entries (default: 86400)
# COMPRESSED_HISTORY_MAX_CONVERSATIONS: max entries in memory store (default: 500)
# COMPRESSED_HISTORY_STORE_BACKEND: "memory" (Phase 3a) or "redis" (Phase 3b, optional)
# CONTEXT_MANAGEMENT_USE_PERSISTED_COMPRESSED_HISTORY=true
# COMPRESSED_HISTORY_TTL_SECONDS=86400
# COMPRESSED_HISTORY_MAX_CONVERSATIONS=500
# COMPRESSED_HISTORY_STORE_BACKEND=memory

# Phase 4: Optional LLM summarization when still over limit after compaction
# CONTEXT_MANAGEMENT_SUMMARIZATION_ENABLED: enable middle-segment summarization (default: false)
# CONTEXT_MANAGEMENT_SUMMARIZATION_MODEL: optional model for summarization (e.g. cheaper); unset = same/cheaper from factory
# SUMMARIZATION_HEAD_MESSAGES: messages to keep at start (default: 2)
# SUMMARIZATION_TAIL_MESSAGES: messages to keep at end (default: 6)
# SUMMARIZATION_TARGET_TOKENS: target tokens for summary (default: 10000)
# CONTEXT_MANAGEMENT_SUMMARIZATION_ENABLED=false
# CONTEXT_MANAGEMENT_SUMMARIZATION_MODEL=
# SUMMARIZATION_HEAD_MESSAGES=2
# SUMMARIZATION_TAIL_MESSAGES=6
# SUMMARIZATION_TARGET_TOKENS=10000

# Multimodal Feature Flag
# Controls availability of image upload and multimodal AI functionality
# Possible values:
#   "auto" (default) - Enable automatically when all GCP vars present
#   "enabled" - Force enable (requires GCP vars)
#   "disabled" - Force disable regardless of GCP vars
isMultimodalEnabled=auto
# Required for multimodal functionality (when enabled)
OBJECT_STORAGE_PROVIDER=gcs
# Storage provider selection: "s3", "gcs", "azure", or "auto" (default: auto)

# GCS config
GCS_PROJECT_ID=
GCS_BUCKET_NAME=

# GCS S3 Interoperability HMAC Keys (required for boto3 access)
GCS_HMAC_ACCESS_KEY=
GCS_HMAC_SECRET_KEY=

# S3 config
S3_BUCKET_NAME=
AWS_REGION=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# Azure Blob Storage config
AZURE_ACCOUNT_NAME=
AZURE_ACCOUNT_KEY=
AZURE_CONTAINER_NAME=

GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
FIREBASE_SERVICE_ACCOUNT=
KNOWLEDGE_GRAPH_URL=
GITHUB_APP_ID=
GITHUB_PRIVATE_KEY=
# Comma-separated GitHub PAT tokens for github.com (e.g., ghp_token1,ghp_token2)
GH_TOKEN_LIST=
TRANSACTION_EMAILS_ENABLED=
EMAIL_FROM_ADDRESS=
RESEND_API_KEY=
ANTHROPIC_API_KEY=
OPENROUTER_API_KEY=
POSTHOG_API_KEY=
POSTHOG_HOST=
FIRECRAWL_API_KEY=

# Phoenix Tracing Configuration
# Set PHOENIX_ENABLED=false to disable tracing
PHOENIX_ENABLED=true
# Phoenix collector endpoint (default: local Phoenix server)
PHOENIX_COLLECTOR_ENDPOINT=http://localhost:6006
# Project name shown in Phoenix UI (default: "potpie-ai")
PHOENIX_PROJECT_NAME=potpie-ai
# GitHub Authentication Configuration
# GH_TOKEN_LIST: Personal Access Tokens for GitHub.com (comma-separated for token pool)
# GITHUB_APP_ID + GITHUB_PRIVATE_KEY: GitHub App credentials (recommended for production)
# CODE_PROVIDER_TOKEN: Token for self-hosted Git servers (GitBucket, GitLab, etc.)
# CODE_PROVIDER_BASE_URL: API base URL for self-hosted Git servers

# Optional: Git provider configuration for self-hosted instances
# Supported providers: github, gitbucket, gitlab, bitbucket, local
# Options: github, gitlab, gitbucket, local
CODE_PROVIDER=github
# e.g., http://localhost:8080/api/v3 for GitBucket, /path/to/repo for local
CODE_PROVIDER_BASE_URL=
# PAT for self-hosted Git server (not needed for local)
CODE_PROVIDER_TOKEN=

# For local provider:
# CODE_PROVIDER=local
# CODE_PROVIDER_BASE_URL=/path/to/local/repository

# For GitHub:
# CODE_PROVIDER=github
# CODE_PROVIDER_BASE_URL=https://api.github.com  # Optional, has default
# CODE_PROVIDER_TOKEN=ghp_xxxxx

# For GitBucket:
# CODE_PROVIDER=gitbucket
# CODE_PROVIDER_BASE_URL=http://localhost:8080/api/v3  # Required
# CODE_PROVIDER_TOKEN=your_token

# For tests
# create a private repo named "potpie-private-test-repo"
# create a file "private_file.txt" with exact contents "This is a private file" in the same repo
# add repo url without github.com
PRIVATE_TEST_REPO_NAME=<yourGithubUsername>/potpie-private-test-repo
